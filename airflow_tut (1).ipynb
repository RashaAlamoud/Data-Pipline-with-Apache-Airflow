{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## AWS Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.\n",
    "\n",
    "Athena is easy to use. Simply point to your data in Amazon S3, define the schema, and start querying using standard SQL. Most results are delivered within seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Udacity lessons:\n",
    "> https://classroom.udacity.com/nanodegrees/nd027-mena-connect/parts/0b05eac1-17a3-4f7e-a43d-9b722fa0f745/modules/96060264-d86d-4b69-9259-0726cc2a336f/lessons/f14bb167-fee8-4a4b-94d3-9ca7fcbabe77/concepts/104d698b-d7bd-4335-b9a4-75c69fd5ed70\n",
    "\n",
    ">https://classroom.udacity.com/nanodegrees/nd027-mena-connect/parts/0b05eac1-17a3-4f7e-a43d-9b722fa0f745/modules/96060264-d86d-4b69-9259-0726cc2a336f/lessons/f14bb167-fee8-4a4b-94d3-9ca7fcbabe77/concepts/7bf7229e-1506-4e27-affd-d215d9e3e949"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Benefits**\n",
    "* **Serverless**: Athena is serverless. You can quickly query your data without having to setup and manage any servers or data warehouses.\n",
    "* **runs standard SQL**: Athena is ideal for quick, ad-hoc querying but it can also handle complex analysis, including large joins, window functions, and arrays.\n",
    "* **Only pay for data scanned** : you pay only for the queries that you run. You are charged $5 per terabyte scanned by your queries. Athena queries data directly in Amazon S3. There are no additional storage charges beyond S3.\n",
    "* **Fast, Really Fast**: Amazon Athena automatically executes queries in parallel, so most results come back within seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# The exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "What we would like to do is simply creating a data pipeline for our third project (Data Warehouse), we will do the trasformation through Athena and then store the results in S3 to be queried later with Athena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "> We will use Airflow to manage the scheduling part, through udacity airflow workspace.\n",
    "\n",
    "> S3 --> Do the Transformation through Athena --> Store the trasformed data again on S3 (partitioned)\n",
    "\n",
    "> Fact table to be updated on a daily basis.\n",
    "\n",
    "> Dimension tables to be updated on a weekly basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step1: create an Athena DB.\n",
    "\n",
    "Name it:  `athena_tut`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step2: Create the tables needed for our raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**On your Athena console, run these two queries**\n",
    "```\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS athena_tut.raw_song_data (\n",
    "  `artist_id` string,\n",
    "  `artist_latitude` float,\n",
    "  `artist_longitude` float,\n",
    "  `artist_location` string,\n",
    "  `artist_name` string,\n",
    "  `song_id` string,\n",
    "  `title` string,\n",
    "  `duration` float,\n",
    "  `year` int \n",
    ")\n",
    "ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\n",
    "WITH SERDEPROPERTIES (\n",
    "  'serialization.format' = '1'\n",
    ") LOCATION 's3://udacity-dend/song_data/'\n",
    "TBLPROPERTIES ('has_encrypted_data'='false');\n",
    "```\n",
    "\n",
    "```\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS athena_tut.raw_log_data (\n",
    "  `artist` string,\n",
    "  `auth` string,\n",
    "  `firstname` string,\n",
    "  `gender` string,\n",
    "  `iteminsession` string,\n",
    "  `length` string,\n",
    "  `level` string,\n",
    "  `location` string,\n",
    "  `method` string,\n",
    "  `page` string,\n",
    "  `registration` string,\n",
    "  `sessionid` string,\n",
    "  `song` string,\n",
    "  `status` string,\n",
    "  `ts` string,\n",
    "  `useragent` string,\n",
    "  `userid` string\n",
    ")\n",
    "ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\n",
    "WITH SERDEPROPERTIES (\n",
    "  'serialization.format' = '1'\n",
    ") LOCATION 's3://udacity-dend/log_data/'\n",
    "TBLPROPERTIES ('has_encrypted_data'='false');\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Test your tables by running queries on them**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step3: Add your AWS Keys to your Airflow workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "> Do not forget to run `/opt/airflow/start.sh` in your workspace terminal in order to run airflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Refer the lesson to create the keys if you haven't before.\n",
    "> https://classroom.udacity.com/nanodegrees/nd027-mena-connect/parts/6a3d5ccd-d0be-4632-ab7b-d20e3bae92d1/modules/445568fc-578d-4d3e-ab9c-2d186728ab22/lessons/21d59f40-6033-40b5-81a2-4a3211d9f46e/concepts/b2d6adbf-3324-45ab-b74b-b5141aa18ef3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Access your Airflow UI, and then paste your keys in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "`Menu -> Admin -> Connections -> aws_default`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step4: Create an S3 bucket for your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step5: Create your pipeline with Airflow (fact  songplay_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We will be creating our tables using Athena CTAS syntax:\n",
    ">https://docs.aws.amazon.com/athena/latest/ug/ctas-examples.html#ctas-example-specify-columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "EXAMPLE : Creating Bucketed and Partitioned Tables \n",
    "```\n",
    "CREATE TABLE  songplay_table\n",
    "WITH (\n",
    "      format = 'CSV', \n",
    "      external_location = 's3://my_athena_results/songplay_table/'\n",
    "      ) \n",
    "      AS\n",
    "    SELECT  DISTINCT(cast(from_unixtime(e.ts/1000) as timestamp))  AS start_time, \n",
    "        e.userId        AS user_id, \n",
    "        e.level         AS level, \n",
    "        s.song_id       AS song_id, \n",
    "        s.artist_id     AS artist_id, \n",
    "        e.sessionId AS session_id, \n",
    "        e.location      AS location, \n",
    "        e.userAgent     AS user_agent\n",
    "    FROM raw_log_data e\n",
    "    JOIN song_data  s   ON (e.song = s.title AND e.artist = s.artist_name)\n",
    "    AND e.page  LIKE  'NextSong'\n",
    "    ;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Use the AWSAthenaOperator to schedule the query using Airflow (on a daily basis): \n",
    "\n",
    "EXAMPLE of such a DAG: \n",
    "```\n",
    "from airflow.models import DAG\n",
    "from airflow.contrib.operators.aws_athena_operator import AWSAthenaOperator\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG(dag_id='simple_athena_query',\n",
    "         schedule_interval='@daily',\n",
    "         start_date=datetime(2019, 5, 21)) as dag:\n",
    " \n",
    "    run_query = AWSAthenaOperator(\n",
    "        task_id='run_query',\n",
    "        query='''\n",
    "        CREATE TABLE  songplay_table\n",
    "        WITH (\n",
    "              format = 'PARQUET', \n",
    "              external_location = 's3://my_athena_results/songplay_table/{{ds}}'\n",
    "              ) \n",
    "              AS\n",
    "   SELECT  DISTINCT(cast(from_unixtime(e.ts/1000) as timestamp))  AS start_time, \n",
    "        e.userId        AS user_id, \n",
    "        e.level         AS level, \n",
    "        s.song_id       AS song_id, \n",
    "        s.artist_id     AS artist_id, \n",
    "        e.sessionId AS session_id, \n",
    "        e.location      AS location, \n",
    "        e.userAgent     AS user_agent\n",
    "    FROM raw_log_data e\n",
    "    JOIN song_data  s   ON (e.song = s.title AND e.artist = s.artist_name)\n",
    "    AND e.page  LIKE  'NextSong'\n",
    "    ;''',\n",
    "        database='athena_tut'\n",
    "    )\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step6: do the same for the other tables (use the query example below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "time dim table (Athena format):\n",
    "```\n",
    " SELECT  DISTINCT(start_time)                AS start_time,\n",
    "            hour(start_time)       AS hour,\n",
    "            day(start_time)        AS day,\n",
    "            week(start_time)      AS week,\n",
    "            month(start_time)    AS month,\n",
    "            year(start_time)       AS year\n",
    "    FROM songplay_table;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step7: Create a sperate dag for dimension tables (weekly run)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "```\n",
    "from airflow.models import DAG\n",
    "from airflow.contrib.operators.aws_athena_operator import AWSAthenaOperator\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG(dag_id='simple_athena_query',\n",
    "         schedule_interval='@weekly',\n",
    "         start_date=datetime(2019, 5, 21)) as dag:\n",
    "\n",
    "    run_query = AWSAthenaOperator(\n",
    "        task_id='run_query',\n",
    "        query='''\n",
    "        CREATE TABLE  dim_time_table\n",
    "        WITH (\n",
    "              format = 'PARQUET', \n",
    "              external_location = 's3://my_athena_results//'\n",
    "              ) \n",
    "              AS\n",
    "    SELECT  DISTINCT(start_time)                AS start_time,\n",
    "            hour(start_time)       AS hour,\n",
    "            day(start_time)        AS day,\n",
    "            week(start_time)      AS week,\n",
    "            month(start_time)    AS month,\n",
    "            year(start_time)       AS year\n",
    "    FROM songplay_table;\n",
    "    ;''',\n",
    "        database='athena_tut'\n",
    "    )\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
